[
    {
        "modelId": "17885302739",
        "modelName": "Pro/deepseek-ai/DeepSeek-V3",
        "mf": "deepseek-ai",
        "desc": "DeepSeek-V3 是一款拥有 6710 亿参数的混合专家（MoE）语言模型，采用多头潜在注意力（MLA）和 DeepSeekMoE 架构，结合无辅助损失的负载平衡策略，优化推理和训练效率。通过在 14.8 万亿高质量tokens上预训练，并进行监督微调和强化学习，DeepSeek-V3 在性能上超越其他开源模型，接近领先闭源模型。",
        "tags": [
            "MoE",
            "671B",
            "64K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/DP-%20ST-Logo.svg",
        "publishTime": {
            "seconds": 1735142400
        },
        "size": 671,
        "contextLen": 65536,
        "price": "8",
        "currency": "¥",
        "operationLabel": [
            "华为云昇腾云服务"
        ],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "DeepSeek-V3 (Pro)",
        "sort": 5500,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 1000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 1000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 1000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 1000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": true,
        "functionCallSupport": true,
        "fimCompletionSupport": true,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": true,
        "supportFT": false,
        "targetModelName": "deepseek-ai/DeepSeek-V3",
        "deprecatedTime": {},
        "filling": "{\"属地\":\"北京市\",\"模型名称\":\"求索对话DeepSeek Chat\",\"备案单位\":\"北京深度求索人工智能基础技术研究有限公司\",\"备案号\":\"Beijing-DeepseekChat-202404280016\",\"备案时间\":\"2024/5/13\"}"
    },
    {
        "modelId": "17885302738",
        "modelName": "Pro/deepseek-ai/DeepSeek-R1",
        "mf": "deepseek-ai",
        "desc": "DeepSeek-R1 是一款强化学习（RL）驱动的推理模型，解决了模型中的重复性和可读性问题。在 RL 之前，DeepSeek-R1 引入了冷启动数据，进一步优化了推理性能。它在数学、代码和推理任务中与 OpenAI-o1 表现相当，并且通过精心设计的训练方法，提升了整体效果。",
        "tags": [
            "推理模型",
            "MoE",
            "671B",
            "64K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/DP-%20ST-Logo.svg",
        "publishTime": {
            "seconds": 1737302400
        },
        "size": 671,
        "contextLen": 65536,
        "price": "16",
        "currency": "¥",
        "operationLabel": [
            "华为云昇腾云服务"
        ],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "DeepSeek-R1 (Pro)",
        "sort": 5600,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 1000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 1000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 1000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 1000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": true,
        "functionCallSupport": true,
        "fimCompletionSupport": true,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": true,
        "supportFT": false,
        "targetModelName": "deepseek-ai/DeepSeek-R1",
        "deprecatedTime": {},
        "filling": "{\"属地\":\"北京市\",\"模型名称\":\"求索对话DeepSeek Chat\",\"备案单位\":\"北京深度求索人工智能基础技术研究有限公司\",\"备案号\":\"Beijing-DeepseekChat-202404280016\",\"备案时间\":\"2024/5/13\"}"
    },
    {
        "modelId": "17885302724",
        "modelName": "deepseek-ai/DeepSeek-R1",
        "mf": "deepseek-ai",
        "desc": "DeepSeek-R1 是一款强化学习（RL）驱动的推理模型，解决了模型中的重复性和可读性问题。在 RL 之前，DeepSeek-R1 引入了冷启动数据，进一步优化了推理性能。它在数学、代码和推理任务中与 OpenAI-o1 表现相当，并且通过精心设计的训练方法，提升了整体效果。",
        "tags": [
            "推理模型",
            "MoE",
            "671B",
            "64K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/DP-%20ST-Logo.svg",
        "publishTime": {
            "seconds": 1737302400
        },
        "size": 671,
        "contextLen": 65536,
        "price": "16",
        "currency": "¥",
        "operationLabel": [
            "华为云昇腾云服务"
        ],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "DeepSeek-R1",
        "sort": 5400,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 10000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1000,
                "TPM": 10000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 1000,
                "TPM": 10000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 1000,
                "TPM": 10000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 1000,
                "TPM": 10000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 1000,
                "TPM": 10000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": true,
        "functionCallSupport": true,
        "fimCompletionSupport": true,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "deepseek-ai/DeepSeek-R1",
        "deprecatedTime": {},
        "filling": "{\"属地\":\"北京市\",\"模型名称\":\"求索对话DeepSeek Chat\",\"备案单位\":\"北京深度求索人工智能基础技术研究有限公司\",\"备案号\":\"Beijing-DeepseekChat-202404280016\",\"备案时间\":\"2024/5/13\"}"
    },
    {
        "modelId": "17885302723",
        "modelName": "deepseek-ai/DeepSeek-V3",
        "mf": "deepseek-ai",
        "desc": "DeepSeek-V3 是一款拥有 6710 亿参数的混合专家（MoE）语言模型，采用多头潜在注意力（MLA）和 DeepSeekMoE 架构，结合无辅助损失的负载平衡策略，优化推理和训练效率。通过在 14.8 万亿高质量tokens上预训练，并进行监督微调和强化学习，DeepSeek-V3 在性能上超越其他开源模型，接近领先闭源模型。",
        "tags": [
            "MoE",
            "671B",
            "64K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/DP-%20ST-Logo.svg",
        "publishTime": {
            "seconds": 1735142400
        },
        "size": 671,
        "contextLen": 65536,
        "price": "8",
        "currency": "¥",
        "operationLabel": [
            "华为云昇腾云服务"
        ],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "DeepSeek-V3",
        "sort": 5200,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 10000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1000,
                "TPM": 10000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 1000,
                "TPM": 10000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 1000,
                "TPM": 10000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 1000,
                "TPM": 10000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 1000,
                "TPM": 10000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": true,
        "functionCallSupport": true,
        "fimCompletionSupport": true,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "deepseek-ai/DeepSeek-V3",
        "deprecatedTime": {},
        "filling": "{\"属地\":\"北京市\",\"模型名称\":\"求索对话DeepSeek Chat\",\"备案单位\":\"北京深度求索人工智能基础技术研究有限公司\",\"备案号\":\"Beijing-DeepseekChat-202404280016\",\"备案时间\":\"2024/5/13\"}"
    },
    {
        "modelId": "17885302787",
        "modelName": "Qwen/QwQ-32B",
        "mf": "QwQ",
        "desc": "QwQ 是 Qwen 系列的推理模型。与传统的指令调优模型相比，QwQ 具备思考和推理能力，能够在下游任务中实现显著增强的性能，尤其是在解决困难问题方面。QwQ-32B 是中型推理模型，能够在与最先进的推理模型（如 DeepSeek-R1、o1-mini）的对比中取得有竞争力的性能。该模型采用 RoPE、SwiGLU、RMSNorm 和 Attention QKV bias 等技术，具有 64 层网络结构和 40 个 Q 注意力头（GQA 架构中 KV 为 8 个）",
        "tags": [
            "推理模型",
            "32B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Tongyi.svg",
        "publishTime": {
            "seconds": 1741190400
        },
        "size": 32,
        "contextLen": 32768,
        "price": "4",
        "currency": "¥",
        "operationLabel": [
            "New"
        ],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "QwQ-32B",
        "sort": 4010,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 40000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 60000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 160000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 2000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": false,
        "functionCallSupport": true,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": false,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "Qwen/QwQ-32B",
        "deprecatedTime": {},
        "filling": "{\"属地\":\"浙江省\",\"模型名称\":\"通义千问大模型\",\"备案单位\":\"阿里巴巴达摩院（杭州）科技有限公司\",\"备案号\":\"ZheJiang-TongYiQianWen-20230901\",\"备案时间\":\"2023/9/12\"}"
    },
    {
        "modelId": "17885302652",
        "modelName": "Qwen/QwQ-32B-Preview",
        "mf": "QwQ",
        "desc": "QwQ-32B-Preview是Qwen 最新的实验性研究模型，专注于提升AI推理能力。通过探索语言混合、递归推理等复杂机制，主要优势包括强大的推理分析能力、数学和编程能力。与此同时，也存在语言切换问题、推理循环、安全性考虑、其他能力方面的差异。",
        "tags": [
            "32B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Tongyi.svg",
        "publishTime": {
            "seconds": 1732723200
        },
        "size": 32,
        "contextLen": 32768,
        "price": "1.26",
        "currency": "¥",
        "operationLabel": [
            "HOT"
        ],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "QwQ-32B-Preview",
        "sort": 3300,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 40000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 60000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 160000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 2000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": true,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "Qwen/QwQ-32B-Preview",
        "deprecatedTime": {},
        "filling": "{\"属地\":\"浙江省\",\"模型名称\":\"通义千问大模型\",\"备案单位\":\"阿里巴巴达摩院（杭州）科技有限公司\",\"备案号\":\"ZheJiang-TongYiQianWen-20230901\",\"备案时间\":\"2023/9/12\"}"
    },
    {
        "modelId": "17885302595",
        "modelName": "Vendor-A/Qwen/Qwen2.5-72B-Instruct",
        "mf": "Qwen2.5",
        "desc": "Qwen2.5-72B-Instruct 是阿里云发布的最新大语言模型系列之一。该 72B 模型在编码和数学等领域具有显著改进的能力。该模型还提供了多语言支持，覆盖超过 29 种语言，包括中文、英文等。模型在指令跟随、理解结构化数据以及生成结构化输出（尤其是 JSON）方面都有显著提升",
        "tags": [
            "72B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Tongyi.svg",
        "publishTime": {
            "seconds": 1726588800
        },
        "size": 72,
        "contextLen": 32768,
        "price": "1",
        "currency": "¥",
        "operationLabel": [
            "国芯"
        ],
        "needAuth": false,
        "onlineDemo": false,
        "type": "text",
        "subType": "chat",
        "DisplayName": "Qwen2.5-72B-Instruct (Vendor-A)",
        "sort": 2,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 20000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 30000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 40000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 250000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "disable",
        "jsonModeSupport": true,
        "functionCallSupport": true,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "Vendor-A/Qwen/Qwen2.5-72B-Instruct",
        "deprecatedTime": {},
        "filling": "{\"属地\":\"浙江省\",\"模型名称\":\"通义千问大模型\",\"备案单位\":\"阿里巴巴达摩院（杭州）科技有限公司\",\"备案号\":\"ZheJiang-TongYiQianWen-20230901\",\"备案时间\":\"2023/9/12\"}"
    },
    {
        "modelId": "17885302516",
        "modelName": "01-ai/Yi-1.5-6B-Chat",
        "mf": "01-ai",
        "desc": "Yi-1.5-6B-Chat 是 Yi-1.5 系列的一个变体，属于开源聊天模型。Yi-1.5 是 Yi 的升级版本，在 500B 个高质量语料上进行了持续预训练，并在 3M 多样化的微调样本上进行了微调。相比于 Yi，Yi-1.5 在编码、数学、推理和指令遵循能力方面表现更强，同时保持了出色的语言理解、常识推理和阅读理解能力。该模型具有 4K、16K 和 32K 的上下文长度版本，预训练总量达到 3.6T 个 token",
        "tags": [
            "Free",
            "6B",
            "4K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Yi.svg",
        "publishTime": {
            "seconds": 1715356800
        },
        "size": 6,
        "contextLen": 4096,
        "price": "0",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": false,
        "type": "text",
        "subType": "chat",
        "DisplayName": "Yi-1.5-6B-Chat (Free)",
        "sort": 2,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "disable",
        "jsonModeSupport": true,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "01-ai/Yi-1.5-6B-Chat",
        "deprecatedTime": {
            "seconds": 1740585600
        },
        "filling": "{\"属地\":\"北京市\",\"模型名称\":\"零一万物大模型\",\"备案单位\":\"北京零一万物科技有限公司\",\"备案号\":\"Beijing-LingYiWanWu-20240102\",\"备案时间\":\"2024/1/17\"}"
    },
    {
        "modelId": "17885302518",
        "modelName": "01-ai/Yi-1.5-34B-Chat-16K",
        "mf": "01-ai",
        "desc": "Yi-1.5-34B-Chat-16K 是 Yi-1.5 系列的一个变体，属于开源聊天模型。Yi-1.5 是 Yi 的升级版本，在 500B 个高质量语料上进行了持续预训练，并在 3M 多样化的微调样本上进行了微调。相比于 Yi，Yi-1.5 在编码、数学、推理和指令遵循能力方面表现更强，同时保持了出色的语言理解、常识推理和阅读理解能力。该模型在大多数基准测试中与更大的模型相当或表现更佳，具有 16K 的上下文长度",
        "tags": [
            "34B",
            "16K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Yi.svg",
        "publishTime": {
            "seconds": 1715702400
        },
        "size": 34,
        "contextLen": 16384,
        "price": "1.26",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": false,
        "type": "text",
        "subType": "chat",
        "DisplayName": "Yi-1.5-34B-Chat-16K",
        "sort": 2,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 40000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 60000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 160000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 2000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "disable",
        "jsonModeSupport": true,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "01-ai/Yi-1.5-34B-Chat-16K",
        "deprecatedTime": {
            "seconds": 1740585600
        },
        "filling": "{\"属地\":\"北京市\",\"模型名称\":\"零一万物大模型\",\"备案单位\":\"北京零一万物科技有限公司\",\"备案号\":\"Beijing-LingYiWanWu-20240102\",\"备案时间\":\"2024/1/17\"}"
    },
    {
        "modelId": "17885302517",
        "modelName": "01-ai/Yi-1.5-9B-Chat-16K",
        "mf": "01-ai",
        "desc": "Yi-1.5-9B-Chat-16K 是 Yi-1.5 系列的一个变体，属于开源聊天模型。Yi-1.5 是 Yi 的升级版本，在 500B 个高质量语料上进行了持续预训练，并在 3M 多样化的微调样本上进行了微调。相比于 Yi，Yi-1.5 在编码、数学、推理和指令遵循能力方面表现更强，同时保持了出色的语言理解、常识推理和阅读理解能力。该模型在同等规模的开源模型中表现最佳",
        "tags": [
            "Free",
            "9B",
            "16K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Yi.svg",
        "publishTime": {
            "seconds": 1715702400
        },
        "size": 9,
        "contextLen": 16384,
        "price": "0",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": false,
        "type": "text",
        "subType": "chat",
        "DisplayName": "Yi-1.5-9B-Chat-16K (Free)",
        "sort": 2,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "disable",
        "jsonModeSupport": true,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "01-ai/Yi-1.5-9B-Chat-16K",
        "deprecatedTime": {
            "seconds": 1740585600
        },
        "filling": "{\"属地\":\"北京市\",\"模型名称\":\"零一万物大模型\",\"备案单位\":\"北京零一万物科技有限公司\",\"备案号\":\"Beijing-LingYiWanWu-20240102\",\"备案时间\":\"2024/1/17\"}"
    },
    {
        "modelId": "17885302780",
        "modelName": "Kwai-Kolors/Kolors",
        "mf": "Kolors",
        "desc": "Kolors 是由快手 Kolors 团队开发的基于潜在扩散的大规模文本到图像生成模型。该模型通过数十亿文本-图像对的训练，在视觉质量、复杂语义准确性以及中英文字符渲染方面展现出显著优势。它不仅支持中英文输入，在理解和生成中文特定内容方面也表现出色",
        "tags": [
            "图生图"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/20250227-194559.png",
        "publishTime": {
            "seconds": 1720195200
        },
        "size": 0,
        "contextLen": 0,
        "price": "0",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": true,
        "type": "image",
        "subType": "text-to-image",
        "DisplayName": "Kolors",
        "sort": 3000,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": -1,
                "TPM": -1,
                "IPM": 2,
                "IPD": 400
            },
            {
                "Level": 1,
                "RPM": -1,
                "TPM": -1,
                "IPM": 2,
                "IPD": 400
            },
            {
                "Level": 2,
                "RPM": -1,
                "TPM": -1,
                "IPM": 2,
                "IPD": 400
            },
            {
                "Level": 3,
                "RPM": -1,
                "TPM": -1,
                "IPM": 2,
                "IPD": 400
            },
            {
                "Level": 4,
                "RPM": -1,
                "TPM": -1,
                "IPM": 2,
                "IPD": 400
            },
            {
                "Level": 5,
                "RPM": -1,
                "TPM": -1,
                "IPM": 2,
                "IPD": 400
            }
        ],
        "priceUnit": "/ M px / Steps",
        "status": "normal",
        "jsonModeSupport": false,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": false,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "Kwai-Kolors/Kolors",
        "deprecatedTime": {},
        "filling": "{\"属地\":\"北京市\",\"模型名称\":\"可图大模型\",\"备案单位\":\"北京快手科技有限公司\",\"备案号\":\"Beijing-KeTu-202404280003\",\"备案时间\":\"2024/5/13\"}"
    },
    {
        "modelId": "17885302558",
        "modelName": "netease-youdao/bce-embedding-base_v1",
        "mf": "netease-youdao",
        "desc": "bce-embedding-base_v1 是由网易有道开发的双语和跨语言嵌入模型。该模型在中英文语义表示和检索任务中表现出色，尤其擅长跨语言场景。它是为检索增强生成（RAG）系统优化的，可以直接应用于教育、医疗、法律等多个领域。该模型不需要特定指令即可使用，能够高效地生成语义向量，为语义搜索和问答系统提供关键支持",
        "tags": [
            "多语言",
            "768 维",
            "279M",
            "512"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/netease-youdao.svg",
        "publishTime": {
            "seconds": 1704211200
        },
        "size": 0,
        "contextLen": 512,
        "price": "0",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": false,
        "type": "text",
        "subType": "embedding",
        "DisplayName": "bce-embedding-base_v1",
        "sort": 3394,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": false,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": false,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "netease-youdao/bce-embedding-base_v1",
        "deprecatedTime": {},
        "filling": "{\"说明\":\"非生成式人工智能服务\"}"
    },
    {
        "modelId": "17885302559",
        "modelName": "BAAI/bge-m3",
        "mf": "BAAI",
        "desc": "BGE-M3 是一个多功能、多语言、多粒度的文本嵌入模型。它支持三种常见的检索功能：密集检索、多向量检索和稀疏检索。该模型可以处理超过100种语言，并且能够处理从短句到长达8192个词元的长文档等不同粒度的输入。BGE-M3在多语言和跨语言检索任务中表现出色，在 MIRACL 和 MKQA 等基准测试中取得了领先结果。它还具有处理长文档检索的能力，在 MLDR 和 NarritiveQA 等数据集上展现了优秀性能",
        "tags": [
            "多语言",
            "1024 维",
            "8K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/BAAI.svg",
        "publishTime": {
            "seconds": 1706371200
        },
        "size": 0,
        "contextLen": 8192,
        "price": "0",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": false,
        "type": "text",
        "subType": "embedding",
        "DisplayName": "bge-m3",
        "sort": 3396,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": false,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": false,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "BAAI/bge-m3",
        "deprecatedTime": {},
        "filling": "{\"说明\":\"非生成式人工智能服务\"}"
    },
    {
        "modelId": "17885302563",
        "modelName": "netease-youdao/bce-reranker-base_v1",
        "mf": "netease-youdao",
        "desc": "bce-reranker-base_v1 是网易有道开发的双语和跨语言重排序模型，支持中文、英文、日文和韩文。该模型在 RAG 系统中用于精确重排检索结果，可以提供有意义的相关性分数，有助于过滤低质量段落。它针对多种 RAG 任务进行了优化，包括翻译、摘要和问答等。该模型无需特定指令即可使用，具有广泛的领域适应性，已在有道的多个产品中得到验证",
        "tags": [
            "多语言",
            "279M",
            "512"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/netease-youdao.svg",
        "publishTime": {
            "seconds": 1704211200
        },
        "size": 0,
        "contextLen": 512,
        "price": "0",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": false,
        "type": "text",
        "subType": "reranker",
        "DisplayName": "bce-reranker-base_v1",
        "sort": 3393,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": false,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": false,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "netease-youdao/bce-reranker-base_v1",
        "deprecatedTime": {},
        "filling": "{\"说明\":\"非生成式人工智能服务\"}"
    },
    {
        "modelId": "17885302564",
        "modelName": "BAAI/bge-reranker-v2-m3",
        "mf": "BAAI",
        "desc": "BAAI/bge-reranker-v2-m3 是一个轻量级的多语言重排序模型。它基于 bge-m3 模型开发，具有强大的多语言能力，易于部署，并且推理速度快。该模型采用查询和文档作为输入，直接输出相似度分数，而不是嵌入向量。它适用于多语言场景，特别是在中文和英文处理方面表现出色",
        "tags": [
            "多语言",
            "568M",
            "8K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/BAAI.svg",
        "publishTime": {
            "seconds": 1710432000
        },
        "size": 0,
        "contextLen": 8192,
        "price": "0",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": false,
        "type": "text",
        "subType": "reranker",
        "DisplayName": "bge-reranker-v2-m3",
        "sort": 3395,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": false,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": false,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "BAAI/bge-reranker-v2-m3",
        "deprecatedTime": {},
        "filling": "{\"说明\":\"非生成式人工智能服务\"}"
    },
    {
        "modelId": "17885302679",
        "modelName": "FunAudioLLM/CosyVoice2-0.5B",
        "mf": "FunAudioLLM",
        "desc": "CosyVoice 2 是一个基于大语言模型的流式语音合成模型，采用统一的流式/非流式框架设计。该模型通过有限标量量化（FSQ）来提升语音 token 的编码本利用率，简化了文本到语音的语言模型架构，并开发了支持不同合成场景的分块感知因果流匹配模型。在流式模式下，模型可实现 150ms 的超低延迟，同时保持与非流式模式几乎相同的合成质量。相比 1.0 版本，发音错误率降低了 30%-50%，MOS 评分从 5.4 提升至 5.53，并支持情感和方言的细粒度控制。支持中文（含方言：粤语、四川话、上海话、天津话等）、英文、日语、韩语，支持跨语言和混合语言场景",
        "tags": [
            "多语言",
            "0.5B"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/FunAudioLLM.png",
        "publishTime": {
            "seconds": 1734278400
        },
        "size": 1,
        "contextLen": 0,
        "price": "50",
        "currency": "¥",
        "operationLabel": [
            "50% OFF"
        ],
        "needAuth": false,
        "onlineDemo": true,
        "type": "audio",
        "subType": "text-to-speech",
        "DisplayName": "FunAudioLLM/CosyVoice2-0.5B",
        "sort": 3400,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 40000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 60000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 160000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 2000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M UTF-8 bytes",
        "status": "normal",
        "jsonModeSupport": false,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": false,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "FunAudioLLM/CosyVoice2-0.5B",
        "deprecatedTime": {},
        "filling": "{\"算法名称\":\"达摩院语音合成算法\",\"主体名称\":\"阿里巴巴达摩院（杭州）科技有限公司\",\"备案号\":\"网信算备330110507206401230051号\",\"说明\":\"非生成式人工智能服务\"}"
    },
    {
        "modelId": "17885302619",
        "modelName": "Tencent/Hunyuan-A52B-Instruct",
        "mf": "hunyuan",
        "desc": "混元大模型（Hunyuan-Large）是业界最大的开源 Transformer 架构 MoE 模型，拥有 3890 亿总参数量和 520 亿激活参数量。该模型采用了高质量合成数据训练、KV 缓存压缩、专家特定学习率缩放等创新技术。在 MMLU、CMMLU、数学推理等多个基准测试中都展现出优异表现",
        "tags": [
            "MoE",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/hunyuan.svg",
        "publishTime": {
            "seconds": 1730908800
        },
        "size": -1,
        "contextLen": 32768,
        "price": "21",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": false,
        "type": "text",
        "subType": "chat",
        "DisplayName": "Hunyuan-A52B-Instruct",
        "sort": 2,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 10000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 15000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 20000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 40000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 125000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "disable",
        "jsonModeSupport": false,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": false,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "Tencent/Hunyuan-A52B-Instruct",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"广东省\", \"模型名称\": \"腾讯混元助手大模型\", \"备案单位\": \"深圳市腾讯计算机系统有限公司\", \"备案号\": \"Guangdong-TencentHunyuan-20230901\", \"备案时间\": \"2023/9/14\"}"
    },
    {
        "modelId": "17885302544",
        "modelName": "Pro/01-ai/Yi-1.5-6B-Chat",
        "mf": "01-ai",
        "desc": "Yi-1.5-6B-Chat 是 Yi-1.5 系列的一个变体，属于开源聊天模型。Yi-1.5 是 Yi 的升级版本，在 500B 个高质量语料上进行了持续预训练，并在 3M 多样化的微调样本上进行了微调。相比于 Yi，Yi-1.5 在编码、数学、推理和指令遵循能力方面表现更强，同时保持了出色的语言理解、常识推理和阅读理解能力。该模型具有 4K、16K 和 32K 的上下文长度版本，预训练总量达到 3.6T 个 token",
        "tags": [
            "6B",
            "4K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Yi.svg",
        "publishTime": {
            "seconds": 1715356800
        },
        "size": 6,
        "contextLen": 4096,
        "price": "0.35",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": false,
        "type": "text",
        "subType": "chat",
        "DisplayName": "Yi-1.5-6B-Chat (Pro)",
        "sort": 2,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 120000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 160000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 320000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 5000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "disable",
        "jsonModeSupport": true,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "01-ai/Yi-1.5-6B-Chat",
        "deprecatedTime": {},
        "filling": "{\"属地\":\"北京市\",\"模型名称\":\"零一万物大模型\",\"备案单位\":\"北京零一万物科技有限公司\",\"备案号\":\"Beijing-LingYiWanWu-20240102\",\"备案时间\":\"2024/1/17\"}"
    },
    {
        "modelId": "17885302543",
        "modelName": "Pro/01-ai/Yi-1.5-9B-Chat-16K",
        "mf": "01-ai",
        "desc": "Yi-1.5-9B-Chat-16K 是 Yi-1.5 系列的一个变体，属于开源聊天模型。Yi-1.5 是 Yi 的升级版本，在 500B 个高质量语料上进行了持续预训练，并在 3M 多样化的微调样本上进行了微调。相比于 Yi，Yi-1.5 在编码、数学、推理和指令遵循能力方面表现更强，同时保持了出色的语言理解、常识推理和阅读理解能力。该模型在同等规模的开源模型中表现最佳",
        "tags": [
            "9B",
            "16K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Yi.svg",
        "publishTime": {
            "seconds": 1715702400
        },
        "size": 9,
        "contextLen": 16384,
        "price": "0.42",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": false,
        "type": "text",
        "subType": "chat",
        "DisplayName": "Yi-1.5-9B-Chat-16K (Pro)",
        "sort": 2,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 120000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 160000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 320000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 5000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "disable",
        "jsonModeSupport": true,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "01-ai/Yi-1.5-9B-Chat-16K",
        "deprecatedTime": {},
        "filling": "{\"属地\":\"北京市\",\"模型名称\":\"零一万物大模型\",\"备案单位\":\"北京零一万物科技有限公司\",\"备案号\":\"Beijing-LingYiWanWu-20240102\",\"备案时间\":\"2024/1/17\"}"
    },
    {
        "modelId": "17885302505",
        "modelName": "Qwen/Qwen1.5-14B-Chat",
        "mf": "Qwen",
        "desc": "Qwen1.5-14B-Chat 是 Qwen2 的 beta 版本，是一个基于 Transformer 架构的 decoder-only 语言模型。该模型经过大规模数据预训练，并通过监督微调和直接偏好优化进行后训练。它采用了 SwiGLU 激活函数、注意力 QKV 偏置等技术，并改进了适用于多种自然语言和代码的分词器。该模型支持 32K 的上下文长度，无需使用 trust_remote_code",
        "tags": [
            "14B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Tongyi.svg",
        "publishTime": {
            "seconds": 1706630400
        },
        "size": 14,
        "contextLen": 32768,
        "price": "0.7",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": false,
        "type": "text",
        "subType": "chat",
        "DisplayName": "【Deprecated】Qwen1.5-14B-Chat",
        "sort": 2,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 40000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 60000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 160000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 2000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "disable",
        "jsonModeSupport": true,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": false,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "Qwen/Qwen1.5-14B-Chat",
        "deprecatedTime": {},
        "filling": "{\n  \"属地\": \"浙江省\",\n  \"模型名称\": \"通义千问大模型\",\n  \"备案单位\": \"阿里巴巴达摩院（杭州）科技有限公司\",\n  \"备案号\": \"ZheJiang-TongYiQianWen-20230901\",\n  \"备案时间\": \"2023/9/12\"\n}"
    },
    {
        "modelId": "17885302513",
        "modelName": "Qwen/Qwen1.5-110B-Chat",
        "mf": "Qwen",
        "desc": "Qwen1.5-110B-Chat 是 Qwen2 的 beta 版本，是一个基于 Transformer 架构的 decoder-only 语言模型。该模型在大规模数据上进行了预训练，并通过监督微调和直接偏好优化进行了后训练。它采用了 SwiGLU 激活函数、注意力 QKV 偏置、组查询注意力等技术，并改进了适用于多种自然语言和代码的分词器。该 110B 模型支持 32K 的上下文长度，无需使用 trust_remote_code，并在对话模型方面显著提升了人类偏好性能",
        "tags": [
            "110B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Tongyi.svg",
        "publishTime": {
            "seconds": 1713974400
        },
        "size": 110,
        "contextLen": 32768,
        "price": "4.13",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": false,
        "type": "text",
        "subType": "chat",
        "DisplayName": "【Deprecated】Qwen1.5-110B-Chat",
        "sort": 2,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 20000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 30000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 40000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 250000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "disable",
        "jsonModeSupport": true,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": false,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "Qwen/Qwen1.5-110B-Chat",
        "deprecatedTime": {},
        "filling": "{\n  \"属地\": \"浙江省\",\n  \"模型名称\": \"通义千问大模型\",\n  \"备案单位\": \"阿里巴巴达摩院（杭州）科技有限公司\",\n  \"备案号\": \"ZheJiang-TongYiQianWen-20230901\",\n  \"备案时间\": \"2023/9/12\"\n}"
    },
    {
        "modelId": "17885302512",
        "modelName": "Qwen/Qwen1.5-7B-Chat",
        "mf": "Qwen",
        "desc": "Qwen1.5-7B-Chat 是 Qwen2 的 beta 版本，是一个基于 Transformer 架构的 decoder-only 语言模型。该模型在大规模数据上进行了预训练，并通过监督微调和直接偏好优化进行了后训练。它采用了 SwiGLU 激活函数、注意力 QKV 偏置等技术，并改进了适用于多种自然语言和代码的分词器。该 7B 模型支持 32K 的上下文长度，无需使用 trust_remote_code，并在对话模型方面显著提升了人类偏好性能",
        "tags": [
            "Free",
            "7B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Tongyi.svg",
        "publishTime": {
            "seconds": 1706630400
        },
        "size": 7,
        "contextLen": 32768,
        "price": "0",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": false,
        "type": "text",
        "subType": "chat",
        "DisplayName": "【Deprecated】Qwen1.5-7B-Chat (Free)",
        "sort": 2,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "disable",
        "jsonModeSupport": true,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": false,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "Qwen/Qwen1.5-7B-Chat",
        "deprecatedTime": {},
        "filling": "{\n  \"属地\": \"浙江省\",\n  \"模型名称\": \"通义千问大模型\",\n  \"备案单位\": \"阿里巴巴达摩院（杭州）科技有限公司\",\n  \"备案号\": \"ZheJiang-TongYiQianWen-20230901\",\n  \"备案时间\": \"2023/9/12\"\n}"
    },
    {
        "modelId": "17885302514",
        "modelName": "Qwen/Qwen1.5-32B-Chat",
        "mf": "Qwen",
        "desc": "Qwen1.5-32B-Chat 是 Qwen2 的 beta 版本，是一个基于 Transformer 架构的 decoder-only 语言模型。该模型在大规模数据上进行了预训练，并通过监督微调和直接偏好优化进行了后训练。它采用了 SwiGLU 激活函数、注意力 QKV 偏置、组查询注意力等技术，并改进了适用于多种自然语言和代码的分词器。该 32B 模型支持 32K 的上下文长度，无需使用 trust_remote_code，并在对话模型方面显著提升了人类偏好性能",
        "tags": [
            "32B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Tongyi.svg",
        "publishTime": {
            "seconds": 1712160000
        },
        "size": 32,
        "contextLen": 32768,
        "price": "1.26",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": false,
        "type": "text",
        "subType": "chat",
        "DisplayName": "【Deprecated】Qwen1.5-32B-Chat",
        "sort": 2,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 40000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 60000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 160000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 2000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "disable",
        "jsonModeSupport": true,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": false,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "Qwen/Qwen1.5-32B-Chat",
        "deprecatedTime": {},
        "filling": "{\n  \"属地\": \"浙江省\",\n  \"模型名称\": \"通义千问大模型\",\n  \"备案单位\": \"阿里巴巴达摩院（杭州）科技有限公司\",\n  \"备案号\": \"ZheJiang-TongYiQianWen-20230901\",\n  \"备案时间\": \"2023/9/12\"\n}"
    },
    {
        "modelId": "17885302540",
        "modelName": "Pro/Qwen/Qwen1.5-7B-Chat",
        "mf": "Qwen",
        "desc": "Qwen1.5-7B-Chat 是 Qwen2 的 beta 版本，是一个基于 Transformer 架构的 decoder-only 语言模型。该模型在大规模数据上进行了预训练，并通过监督微调和直接偏好优化进行了后训练。它采用了 SwiGLU 激活函数、注意力 QKV 偏置等技术，并改进了适用于多种自然语言和代码的分词器。该 7B 模型支持 32K 的上下文长度，无需使用 trust_remote_code，并在对话模型方面显著提升了人类偏好性能",
        "tags": [
            "7B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Tongyi.svg",
        "publishTime": {
            "seconds": 1706630400
        },
        "size": 7,
        "contextLen": 32768,
        "price": "0.35",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": false,
        "type": "text",
        "subType": "chat",
        "DisplayName": "【Deprecated】Qwen1.5-7B-Chat (Pro)",
        "sort": 2,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 120000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 160000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 320000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 5000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "disable",
        "jsonModeSupport": true,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": false,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "Qwen/Qwen1.5-7B-Chat",
        "deprecatedTime": {},
        "filling": "{\n  \"属地\": \"浙江省\",\n  \"模型名称\": \"通义千问大模型\",\n  \"备案单位\": \"阿里巴巴达摩院（杭州）科技有限公司\",\n  \"备案号\": \"ZheJiang-TongYiQianWen-20230901\",\n  \"备案时间\": \"2023/9/12\"\n}"
    },
    {
        "modelId": "17885302658",
        "modelName": "tencent/HunyuanVideo",
        "mf": "hunyuan",
        "desc": "HunyuanVideo 是腾讯推出的开源视频生成基础模型，拥有超过 130 亿参数，是目前最大的开源视频生成模型。该模型采用统一的图像和视频生成架构，集成了数据整理、图像-视频联合模型训练和高效基础设施等关键技术。模型使用多模态大语言模型作为文本编码器，通过 3D VAE 进行空间-时间压缩，并提供提示词重写功能。根据专业人工评估结果，HunyuanVideo 在文本对齐、运动质量和视觉质量等方面的表现优于现有最先进的模型",
        "tags": [
            "13B"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/hunyuan.svg",
        "publishTime": {
            "seconds": 1733155200
        },
        "size": 13,
        "contextLen": 0,
        "price": "0.7",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": true,
        "type": "video",
        "subType": "text-to-video",
        "DisplayName": "HunyuanVideo",
        "sort": 3500,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": -1,
                "TPM": -1,
                "IPM": 5,
                "IPD": 200
            },
            {
                "Level": 1,
                "RPM": -1,
                "TPM": -1,
                "IPM": 5,
                "IPD": 300
            },
            {
                "Level": 2,
                "RPM": -1,
                "TPM": -1,
                "IPM": 5,
                "IPD": 400
            },
            {
                "Level": 3,
                "RPM": -1,
                "TPM": -1,
                "IPM": 5,
                "IPD": 500
            },
            {
                "Level": 4,
                "RPM": -1,
                "TPM": -1,
                "IPM": 5,
                "IPD": 600
            },
            {
                "Level": 5,
                "RPM": -1,
                "TPM": -1,
                "IPM": 5,
                "IPD": 3000
            }
        ],
        "priceUnit": "/ Video",
        "status": "normal",
        "jsonModeSupport": false,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": false,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "tencent/HunyuanVideo",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"广东省\", \"模型名称\": \"腾讯混元助手大模型\", \"备案单位\": \"深圳市腾讯计算机系统有限公司\", \"备案号\": \"Guangdong-TencentHunyuan-20230901\", \"备案时间\": \"2023/9/14\"}"
    },
    {
        "modelId": "17885302703",
        "modelName": "tencent/HunyuanVideo-HD",
        "mf": "hunyuan",
        "desc": "HunyuanVideo 是腾讯推出的开源视频生成基础模型，拥有超过 130 亿参数，是目前最大的开源视频生成模型。该模型采用统一的图像和视频生成架构，集成了数据整理、图像-视频联合模型训练和高效基础设施等关键技术。模型使用多模态大语言模型作为文本编码器，通过 3D VAE 进行空间-时间压缩，并提供提示词重写功能。根据专业人工评估结果，HunyuanVideo 在文本对齐、运动质量和视觉质量等方面的表现优于现有最先进的模型",
        "tags": [
            "13B"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/hunyuan.svg",
        "publishTime": {
            "seconds": 1733155200
        },
        "size": 13,
        "contextLen": 0,
        "price": "2.8",
        "currency": "¥",
        "operationLabel": [
            "New"
        ],
        "needAuth": false,
        "onlineDemo": true,
        "type": "video",
        "subType": "text-to-video",
        "DisplayName": "HunyuanVideo (HD)",
        "sort": 3600,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": -1,
                "TPM": -1,
                "IPM": 5,
                "IPD": 200
            },
            {
                "Level": 1,
                "RPM": -1,
                "TPM": -1,
                "IPM": 5,
                "IPD": 300
            },
            {
                "Level": 2,
                "RPM": -1,
                "TPM": -1,
                "IPM": 5,
                "IPD": 400
            },
            {
                "Level": 3,
                "RPM": -1,
                "TPM": -1,
                "IPM": 5,
                "IPD": 500
            },
            {
                "Level": 4,
                "RPM": -1,
                "TPM": -1,
                "IPM": 5,
                "IPD": 600
            },
            {
                "Level": 5,
                "RPM": -1,
                "TPM": -1,
                "IPM": 5,
                "IPD": 3000
            }
        ],
        "priceUnit": "/ Video",
        "status": "normal",
        "jsonModeSupport": false,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": false,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "tencent/HunyuanVideo-HD",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"广东省\", \"模型名称\": \"腾讯混元助手大模型\", \"备案单位\": \"深圳市腾讯计算机系统有限公司\", \"备案号\": \"Guangdong-TencentHunyuan-20230901\", \"备案时间\": \"2023/9/14\"}"
    },
    {
        "modelId": "17885302698",
        "modelName": "Qwen/QVQ-72B-Preview",
        "mf": "QVQ",
        "desc": "QVQ-72B-Preview 是由 Qwen 团队开发的专注于视觉推理能力的研究型模型。该模型在多项基准测试中表现突出，在 MMMU 测试中达到了 70.3% 的卓越成绩，在 MathVista 达到 71.4% 的优异表现，展现了其在多学科理解和数学视觉推理方面的卓越能力。作为专门针对视觉推理优化的模型，QVQ-72B-Preview 在复杂场景理解和解决视觉相关的数学问题方面具有独特优势",
        "tags": [
            "72B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Tongyi.svg",
        "publishTime": {
            "seconds": 1735056000
        },
        "size": 72,
        "contextLen": 32768,
        "price": "9.9",
        "currency": "¥",
        "operationLabel": [
            "New"
        ],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "QVQ-72B-Preview",
        "sort": 4000,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 20000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 30000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 40000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 250000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": true,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": true,
        "vlm": true,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "Qwen/QVQ-72B-Preview",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"浙江省\", \"模型名称\": \"通义千问大模型\", \"备案单位\": \"阿里巴巴达摩院（杭州）科技有限公司\", \"备案号\": \"ZheJiang-TongYiQianWen-20230901\", \"备案时间\": \"2023/9/12\"}"
    },
    {
        "modelId": "17885302734",
        "modelName": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
        "mf": "deepseek-ai",
        "desc": "DeepSeek-R1-Distill-Qwen-1.5B 是基于 Qwen2.5-Math-1.5B 通过知识蒸馏得到的模型。该模型使用 DeepSeek-R1 生成的 80 万个精选样本进行微调，在多个基准测试中展现出不错的性能。作为一个轻量级模型，在 MATH-500 上达到了 83.9% 的准确率，在 AIME 2024 上达到了 28.9% 的通过率，在 CodeForces 上获得了 954 的评分，显示出超出其参数规模的推理能力",
        "tags": [
            "推理模型",
            "1.5B",
            "32K",
            "Math"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/DeepSeek.svg",
        "publishTime": {
            "seconds": 1737302400
        },
        "size": 2,
        "contextLen": 32768,
        "price": "0",
        "currency": "¥",
        "operationLabel": [
            "New"
        ],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "DeepSeek-R1-Distill-Qwen-1.5B (Free)",
        "sort": 4110,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": false,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": false,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"北京市\", \"模型名称\": \"求索对话DeepSeek Chat\", \"备案单位\": \"北京深度求索人工智能基础技术研究有限公司\", \"备案号\": \"Beijing-DeepseekChat-202404280016\", \"备案时间\": \"2024/5/13\"}"
    },
    {
        "modelId": "17885302726",
        "modelName": "Pro/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
        "mf": "deepseek-ai",
        "desc": "DeepSeek-R1-Distill-Qwen-7B 是基于 Qwen2.5-Math-7B 通过知识蒸馏得到的模型。该模型使用 DeepSeek-R1 生成的 80 万个精选样本进行微调，展现出优秀的推理能力。在多个基准测试中表现出色，其中在 MATH-500 上达到了 92.8% 的准确率，在 AIME 2024 上达到了 55.5% 的通过率，在 CodeForces 上获得了 1189 的评分，作为 7B 规模的模型展示了较强的数学和编程能力",
        "tags": [
            "推理模型",
            "7B",
            "32K",
            "Math"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/DeepSeek.svg",
        "publishTime": {
            "seconds": 1737302400
        },
        "size": 7,
        "contextLen": 32768,
        "price": "0.35",
        "currency": "¥",
        "operationLabel": [
            "New"
        ],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "DeepSeek-R1-Distill-Qwen-7B (Pro)",
        "sort": 260,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 120000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 160000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 320000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 5000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": false,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": false,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"北京市\", \"模型名称\": \"求索对话DeepSeek Chat\", \"备案单位\": \"北京深度求索人工智能基础技术研究有限公司\", \"备案号\": \"Beijing-DeepseekChat-202404280016\", \"备案时间\": \"2024/5/13\"}"
    },
    {
        "modelId": "17885302725",
        "modelName": "Pro/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
        "mf": "deepseek-ai",
        "desc": "DeepSeek-R1-Distill-Qwen-1.5B 是基于 Qwen2.5-Math-1.5B 通过知识蒸馏得到的模型。该模型使用 DeepSeek-R1 生成的 80 万个精选样本进行微调，在多个基准测试中展现出不错的性能。作为一个轻量级模型，在 MATH-500 上达到了 83.9% 的准确率，在 AIME 2024 上达到了 28.9% 的通过率，在 CodeForces 上获得了 954 的评分，显示出超出其参数规模的推理能力",
        "tags": [
            "推理模型",
            "1.5B",
            "32K",
            "Math"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/DeepSeek.svg",
        "publishTime": {
            "seconds": 1737302400
        },
        "size": 2,
        "contextLen": 32768,
        "price": "0.14",
        "currency": "¥",
        "operationLabel": [
            "New"
        ],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "DeepSeek-R1-Distill-Qwen-1.5B (Pro)",
        "sort": 250,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 120000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 160000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 320000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 5000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": false,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": false,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"北京市\", \"模型名称\": \"求索对话DeepSeek Chat\", \"备案单位\": \"北京深度求索人工智能基础技术研究有限公司\", \"备案号\": \"Beijing-DeepseekChat-202404280016\", \"备案时间\": \"2024/5/13\"}"
    },
    {
        "modelId": "17885302728",
        "modelName": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
        "mf": "deepseek-ai",
        "desc": "DeepSeek-R1-Distill-Qwen-14B 是基于 Qwen2.5-14B 通过知识蒸馏得到的模型。该模型使用 DeepSeek-R1 生成的 80 万个精选样本进行微调，展现出优秀的推理能力。在多个基准测试中表现出色，其中在 MATH-500 上达到了 93.9% 的准确率，在 AIME 2024 上达到了 69.7% 的通过率，在 CodeForces 上获得了 1481 的评分，显示出在数学和编程领域的强大实力",
        "tags": [
            "推理模型",
            "14B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/DeepSeek.svg",
        "publishTime": {
            "seconds": 1737302400
        },
        "size": 14,
        "contextLen": 32768,
        "price": "0.7",
        "currency": "¥",
        "operationLabel": [
            "New"
        ],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "DeepSeek-R1-Distill-Qwen-14B",
        "sort": 4165,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 40000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 60000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 160000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 2000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": false,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": false,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"北京市\", \"模型名称\": \"求索对话DeepSeek Chat\", \"备案单位\": \"北京深度求索人工智能基础技术研究有限公司\", \"备案号\": \"Beijing-DeepseekChat-202404280016\", \"备案时间\": \"2024/5/13\"}"
    },
    {
        "modelId": "17885302729",
        "modelName": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
        "mf": "deepseek-ai",
        "desc": "DeepSeek-R1-Distill-Qwen-32B 是基于 Qwen2.5-32B 通过知识蒸馏得到的模型。该模型使用 DeepSeek-R1 生成的 80 万个精选样本进行微调，在数学、编程和推理等多个领域展现出卓越的性能。在 AIME 2024、MATH-500、GPQA Diamond 等多个基准测试中都取得了优异成绩，其中在 MATH-500 上达到了 94.3% 的准确率，展现出强大的数学推理能力",
        "tags": [
            "推理模型",
            "32B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/DeepSeek.svg",
        "publishTime": {
            "seconds": 1737302400
        },
        "size": 32,
        "contextLen": 32768,
        "price": "1.26",
        "currency": "¥",
        "operationLabel": [
            "New"
        ],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "DeepSeek-R1-Distill-Qwen-32B",
        "sort": 4170,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 40000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 60000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 160000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 2000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": false,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": false,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"北京市\", \"模型名称\": \"求索对话DeepSeek Chat\", \"备案单位\": \"北京深度求索人工智能基础技术研究有限公司\", \"备案号\": \"Beijing-DeepseekChat-202404280016\", \"备案时间\": \"2024/5/13\"}"
    },
    {
        "modelId": "17885302736",
        "modelName": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
        "mf": "deepseek-ai",
        "desc": "DeepSeek-R1-Distill-Qwen-7B 是基于 Qwen2.5-Math-7B 通过知识蒸馏得到的模型。该模型使用 DeepSeek-R1 生成的 80 万个精选样本进行微调，展现出优秀的推理能力。在多个基准测试中表现出色，其中在 MATH-500 上达到了 92.8% 的准确率，在 AIME 2024 上达到了 55.5% 的通过率，在 CodeForces 上获得了 1189 的评分，作为 7B 规模的模型展示了较强的数学和编程能力",
        "tags": [
            "推理模型",
            "7B",
            "32K",
            "Math"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/DeepSeek.svg",
        "publishTime": {
            "seconds": 1737302400
        },
        "size": 7,
        "contextLen": 32768,
        "price": "0",
        "currency": "¥",
        "operationLabel": [
            "New"
        ],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "DeepSeek-R1-Distill-Qwen-7B (Free)",
        "sort": 4150,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": false,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": false,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"北京市\", \"模型名称\": \"求索对话DeepSeek Chat\", \"备案单位\": \"北京深度求索人工智能基础技术研究有限公司\", \"备案号\": \"Beijing-DeepseekChat-202404280016\", \"备案时间\": \"2024/5/13\"}"
    },
    {
        "modelId": "17885302689",
        "modelName": "deepseek-ai/deepseek-vl2",
        "mf": "deepseek-ai",
        "desc": "DeepSeek-VL2 是一个基于 DeepSeekMoE-27B 开发的混合专家（MoE）视觉语言模型，采用稀疏激活的 MoE 架构，在仅激活 4.5B 参数的情况下实现了卓越性能。该模型在视觉问答、光学字符识别、文档/表格/图表理解和视觉定位等多个任务中表现优异，与现有的开源稠密模型和基于 MoE 的模型相比，在使用相同或更少的激活参数的情况下，实现了具有竞争力的或最先进的性能表现",
        "tags": [
            "MoE",
            "27B",
            "4K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/DeepSeek.svg",
        "publishTime": {
            "seconds": 1734019200
        },
        "size": -1,
        "contextLen": 4096,
        "price": "0.99",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "deepseek-vl2",
        "sort": 2600,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 40000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 60000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 160000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 2000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": true,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": true,
        "vlm": true,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "deepseek-ai/deepseek-vl2",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"北京市\", \"模型名称\": \"求索对话DeepSeek Chat\", \"备案单位\": \"北京深度求索人工智能基础技术研究有限公司\", \"备案号\": \"Beijing-DeepseekChat-202404280016\", \"备案时间\": \"2024/5/13\"}"
    },
    {
        "modelId": "17885302587",
        "modelName": "Qwen/Qwen2.5-72B-Instruct-128K",
        "mf": "Qwen2.5",
        "desc": "Qwen2.5-72B-Instruct 是阿里云发布的最新大语言模型系列之一。该 72B 模型在编码和数学等领域具有显著改进的能力。它支持长达 128K tokens 的上下文。该模型还提供了多语言支持，覆盖超过 29 种语言，包括中文、英文等。模型在指令跟随、理解结构化数据以及生成结构化输出（尤其是 JSON）方面都有显著提升",
        "tags": [
            "72B",
            "128K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Tongyi.svg",
        "publishTime": {
            "seconds": 1726588800
        },
        "size": 72,
        "contextLen": 131072,
        "price": "4.13",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "Qwen2.5-72B-Instruct-128K",
        "sort": 2610,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 20000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 30000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 40000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 250000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": true,
        "functionCallSupport": true,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "Qwen/Qwen2.5-72B-Instruct-128K",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"浙江省\", \"模型名称\": \"通义千问大模型\", \"备案单位\": \"阿里巴巴达摩院（杭州）科技有限公司\", \"备案号\": \"ZheJiang-TongYiQianWen-20230901\", \"备案时间\": \"2023/9/12\"}"
    },
    {
        "modelId": "17885302520",
        "modelName": "deepseek-ai/DeepSeek-V2-Chat",
        "mf": "deepseek-ai",
        "desc": "DeepSeek-V2 是一个强大、经济高效的混合专家（MoE）语言模型。它在 8.1 万亿个 token 的高质量语料库上进行了预训练，并通过监督微调（SFT）和强化学习（RL）进一步提升了模型能力。与 DeepSeek 67B 相比， DeepSeek-V2 在性能更强的同时，节省了 42.5% 的训练成本，减少了 93.3% 的 KV 缓存，并将最大生成吞吐量提高到了 5.76 倍。该模型支持 128k 的上下文长度，在标准基准测试和开放式生成评估中都表现出色",
        "tags": [
            "MoE",
            "236B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/DeepSeek.svg",
        "publishTime": {
            "seconds": 1714924800
        },
        "size": -1,
        "contextLen": 32768,
        "price": "1.33",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": false,
        "type": "text",
        "subType": "chat",
        "DisplayName": "DeepSeek-V2-Chat",
        "sort": 2,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 10000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 15000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 20000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 40000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 125000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "disable",
        "jsonModeSupport": true,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "deepseek-ai/DeepSeek-V2-Chat",
        "deprecatedTime": {},
        "filling": "{\"属地\":\"北京市\",\"模型名称\":\"求索对话DeepSeek Chat\",\"备案单位\":\"北京深度求索人工智能基础技术研究有限公司\",\"备案号\":\"Beijing-DeepseekChat-202404280016\",\"备案时间\":\"2024/5/13\"}"
    },
    {
        "modelId": "17885302523",
        "modelName": "Qwen/Qwen2-72B-Instruct",
        "mf": "Qwen2",
        "desc": "Qwen2-72B-Instruct 是 Qwen2 系列中的指令微调大语言模型，参数规模为 72B。该模型基于 Transformer 架构，采用了 SwiGLU 激活函数、注意力 QKV 偏置和组查询注意力等技术。它能够处理大规模输入。该模型在语言理解、生成、多语言能力、编码、数学和推理等多个基准测试中表现出色，超越了大多数开源模型，并在某些任务上展现出与专有模型相当的竞争力",
        "tags": [
            "72B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Tongyi.svg",
        "publishTime": {
            "seconds": 1716825600
        },
        "size": 72,
        "contextLen": 32768,
        "price": "4.13",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": false,
        "type": "text",
        "subType": "chat",
        "DisplayName": "Qwen2-72B-Instruct",
        "sort": 2,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 20000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 30000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 40000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 250000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "disable",
        "jsonModeSupport": true,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "Qwen/Qwen2-72B-Instruct",
        "deprecatedTime": {},
        "filling": "{\"属地\":\"浙江省\",\"模型名称\":\"通义千问大模型\",\"备案单位\":\"阿里巴巴达摩院（杭州）科技有限公司\",\"备案号\":\"ZheJiang-TongYiQianWen-20230901\",\"备案时间\":\"2023/9/12\"}"
    },
    {
        "modelId": "17885302571",
        "modelName": "Vendor-A/Qwen/Qwen2-72B-Instruct",
        "mf": "Qwen2",
        "desc": "Qwen2-72B-Instruct 是 Qwen2 系列中的指令微调大语言模型，参数规模为 72B。该模型基于 Transformer 架构，采用了 SwiGLU 激活函数、注意力 QKV 偏置和组查询注意力等技术。它能够处理大规模输入。该模型在语言理解、生成、多语言能力、编码、数学和推理等多个基准测试中表现出色，超越了大多数开源模型，并在某些任务上展现出与专有模型相当的竞争力",
        "tags": [
            "72B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Tongyi.svg",
        "publishTime": {
            "seconds": 1716825600
        },
        "size": 72,
        "contextLen": 32768,
        "price": "1",
        "currency": "¥",
        "operationLabel": [
            "国芯"
        ],
        "needAuth": false,
        "onlineDemo": false,
        "type": "text",
        "subType": "chat",
        "DisplayName": "Qwen2-72B-Instruct (Vendor-A)",
        "sort": 2,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 20000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 30000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 40000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 250000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "disable",
        "jsonModeSupport": false,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "Vendor-A/Qwen/Qwen2-72B-Instruct",
        "deprecatedTime": {},
        "filling": "{\"属地\":\"浙江省\",\"模型名称\":\"通义千问大模型\",\"备案单位\":\"阿里巴巴达摩院（杭州）科技有限公司\",\"备案号\":\"ZheJiang-TongYiQianWen-20230901\",\"备案时间\":\"2023/9/12\"}"
    },
    {
        "modelId": "17885302525",
        "modelName": "Qwen/Qwen2-57B-A14B-Instruct",
        "mf": "Qwen2",
        "desc": "Qwen2-57B-A14B-Instruct 是 Qwen2 系列中的指令微调大语言模型，采用混合专家（Mixture-of-Experts）架构，总参数量为 57B，激活参数为 14B。该模型基于 Transformer 架构，使用了 SwiGLU 激活函数、注意力 QKV 偏置和组查询注意力等技术。它能够处理大规模输入。在语言理解、生成、多语言能力、编码、数学和推理等多个基准测试中，该模型表现出色，超越了大多数开源模型，并在某些任务上展现出与专有模型相当的竞争力",
        "tags": [
            "MoE",
            "57B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Tongyi.svg",
        "publishTime": {
            "seconds": 1717430400
        },
        "size": -1,
        "contextLen": 32768,
        "price": "1.26",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": false,
        "type": "text",
        "subType": "chat",
        "DisplayName": "Qwen2-57B-A14B-Instruct",
        "sort": 2,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 20000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 30000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 40000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 250000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "disable",
        "jsonModeSupport": true,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "Qwen/Qwen2-57B-A14B-Instruct",
        "deprecatedTime": {},
        "filling": "{\"属地\":\"浙江省\",\"模型名称\":\"通义千问大模型\",\"备案单位\":\"阿里巴巴达摩院（杭州）科技有限公司\",\"备案号\":\"ZheJiang-TongYiQianWen-20230901\",\"备案时间\":\"2023/9/12\"}"
    },
    {
        "modelId": "17885302577",
        "modelName": "Qwen/Qwen2.5-Math-72B-Instruct",
        "mf": "Qwen2.5",
        "desc": "Qwen2.5-Math-72B 是阿里云发布的 Qwen2.5-Math 系列数学大语言模型之一。该模型支持使用思维链（CoT）和工具集成推理（TIR）方法解决中文和英文数学问题。相比前代 Qwen2-Math 系列，Qwen2.5-Math 系列在中英文数学基准测试中取得了显著的性能提升。该模型在处理精确计算、符号操作和算法操作方面表现出色，尤其适合解决复杂的数学和算法推理任务",
        "tags": [
            "Math",
            "72B",
            "4K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Tongyi.svg",
        "publishTime": {
            "seconds": 1726761600
        },
        "size": 72,
        "contextLen": 4096,
        "price": "4.13",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": false,
        "type": "text",
        "subType": "chat",
        "DisplayName": "Qwen2.5-Math-72B-Instruct",
        "sort": 2,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 20000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 30000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 40000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 250000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "disable",
        "jsonModeSupport": true,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "Qwen/Qwen2.5-Math-72B-Instruct",
        "deprecatedTime": {},
        "filling": "{\"属地\":\"浙江省\",\"模型名称\":\"通义千问大模型\",\"备案单位\":\"阿里巴巴达摩院（杭州）科技有限公司\",\"备案号\":\"ZheJiang-TongYiQianWen-20230901\",\"备案时间\":\"2023/9/12\"}"
    },
    {
        "modelId": "17885302570",
        "modelName": "deepseek-ai/DeepSeek-V2.5",
        "mf": "deepseek-ai",
        "desc": "DeepSeek-V2.5-1210 是 DeepSeek-V2.5 的升级版本，在多个能力方面都有显著提升。在数学能力方面，其在 MATH-500 基准测试上的表现从 74.8% 提升至 82.8%；在编程方面，LiveCodebench 基准测试的准确率从 29.2% 提升至 34.38%。同时在写作和推理方面也有明显改进。模型支持函数调用、JSON 输出和填充式补全等多种功能",
        "tags": [
            "MoE",
            "236B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/DeepSeek.svg",
        "publishTime": {
            "seconds": 1733760000
        },
        "size": -1,
        "contextLen": 32768,
        "price": "1.33",
        "currency": "¥",
        "operationLabel": [
            "1210"
        ],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "DeepSeek-V2.5",
        "sort": 3700,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 10000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 15000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 20000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 40000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 125000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": true,
        "functionCallSupport": true,
        "fimCompletionSupport": true,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "deepseek-ai/DeepSeek-V2.5",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"北京市\", \"模型名称\": \"求索对话DeepSeek Chat\", \"备案单位\": \"北京深度求索人工智能基础技术研究有限公司\", \"备案号\": \"Beijing-DeepseekChat-202404280016\", \"备案时间\": \"2024/5/13\"}"
    },
    {
        "modelId": "17885302575",
        "modelName": "Qwen/Qwen2.5-32B-Instruct",
        "mf": "Qwen2.5",
        "desc": "Qwen2.5-32B-Instruct 是阿里云发布的最新大语言模型系列之一。该 32B 模型在编码和数学等领域具有显著改进的能力。该模型还提供了多语言支持，覆盖超过 29 种语言，包括中文、英文等。模型在指令跟随、理解结构化数据以及生成结构化输出（尤其是 JSON）方面都有显著提升",
        "tags": [
            "32B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Tongyi.svg",
        "publishTime": {
            "seconds": 1726675200
        },
        "size": 32,
        "contextLen": 32768,
        "price": "1.26",
        "currency": "¥",
        "operationLabel": [
            "HOT"
        ],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "Qwen2.5-32B-Instruct",
        "sort": 2590,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 40000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 60000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 160000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 2000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": true,
        "functionCallSupport": true,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": true,
        "targetModelName": "Qwen/Qwen2.5-32B-Instruct",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"浙江省\", \"模型名称\": \"通义千问大模型\", \"备案单位\": \"阿里巴巴达摩院（杭州）科技有限公司\", \"备案号\": \"ZheJiang-TongYiQianWen-20230901\", \"备案时间\": \"2023/9/12\"}"
    },
    {
        "modelId": "17885302574",
        "modelName": "Qwen/Qwen2.5-14B-Instruct",
        "mf": "Qwen2.5",
        "desc": "Qwen2.5-14B-Instruct 是阿里云发布的最新大语言模型系列之一。该 14B 模型在编码和数学等领域具有显著改进的能力。该模型还提供了多语言支持，覆盖超过 29 种语言，包括中文、英文等。模型在指令跟随、理解结构化数据以及生成结构化输出（尤其是 JSON）方面都有显著提升",
        "tags": [
            "14B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Tongyi.svg",
        "publishTime": {
            "seconds": 1726588800
        },
        "size": 14,
        "contextLen": 32768,
        "price": "0.7",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "Qwen2.5-14B-Instruct",
        "sort": 2580,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 40000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 60000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 160000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 2000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": true,
        "functionCallSupport": true,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": true,
        "targetModelName": "Qwen/Qwen2.5-14B-Instruct",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"浙江省\", \"模型名称\": \"通义千问大模型\", \"备案单位\": \"阿里巴巴达摩院（杭州）科技有限公司\", \"备案号\": \"ZheJiang-TongYiQianWen-20230901\", \"备案时间\": \"2023/9/12\"}"
    },
    {
        "modelId": "17885302644",
        "modelName": "Pro/BAAI/bge-reranker-v2-m3",
        "mf": "BAAI",
        "desc": "BAAI/bge-reranker-v2-m3 是一个轻量级的多语言重排序模型。它基于 bge-m3 模型开发，具有强大的多语言能力，易于部署，并且推理速度快。该模型采用查询和文档作为输入，直接输出相似度分数，而不是嵌入向量。它适用于多语言场景，特别是在中文和英文处理方面表现出色",
        "tags": [
            "多语言",
            "568M",
            "8K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/BAAI.svg",
        "publishTime": {
            "seconds": 1710432000
        },
        "size": 0,
        "contextLen": 8192,
        "price": "0.07",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": false,
        "type": "text",
        "subType": "reranker",
        "DisplayName": "bge-reranker-v2-m3 (Pro)",
        "sort": 207,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 2000,
                "TPM": 1000000,
                "IPM": 1,
                "IPD": 1440
            },
            {
                "Level": 1,
                "RPM": 3000,
                "TPM": 1000000,
                "IPM": 1,
                "IPD": 1440
            },
            {
                "Level": 2,
                "RPM": 5000,
                "TPM": 1000000,
                "IPM": 1,
                "IPD": 1440
            },
            {
                "Level": 3,
                "RPM": 5000,
                "TPM": 5000000,
                "IPM": 1,
                "IPD": 1440
            },
            {
                "Level": 4,
                "RPM": 10000,
                "TPM": 5000000,
                "IPM": 1,
                "IPD": 1440
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 10000000,
                "IPM": 1,
                "IPD": 1440
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": false,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": false,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "BAAI/bge-reranker-v2-m3",
        "deprecatedTime": {},
        "filling": "{\"说明\": \"非生成式人工智能服务\"}"
    },
    {
        "modelId": "17885302528",
        "modelName": "deepseek-ai/DeepSeek-Coder-V2-Instruct",
        "mf": "deepseek-ai",
        "desc": "DeepSeek-Coder-V2 是一个开源的混合专家（MoE）代码语言模型，在代码相关任务中达到了与 GPT4-Turbo 相当的性能。它是在 DeepSeek-V2 的中间检查点基础上，通过额外 6 万亿个 token 的预训练而来。该模型显著提升了编码和数学推理能力，同时保持了通用语言任务的性能。相比 DeepSeek-Coder-33B，它在各方面都有显著进步，支持的编程语言从 86 种扩展到 338 种，上下文长度从 16K 扩展到 128K。在标准基准评估中，DeepSeek-Coder-V2 在编码和数学基准测试中的表现超过了 GPT4-Turbo、Claude 3 Opus 和 Gemini 1.5 Pro 等闭源模型",
        "tags": [
            "Coder",
            "MoE",
            "236B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/DeepSeek.svg",
        "publishTime": {
            "seconds": 1718294400
        },
        "size": -1,
        "contextLen": 32768,
        "price": "1.33",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": false,
        "type": "text",
        "subType": "chat",
        "DisplayName": "DeepSeek-Coder-V2-Instruct",
        "sort": 2,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 10000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 15000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 20000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 40000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 125000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "disable",
        "jsonModeSupport": true,
        "functionCallSupport": false,
        "fimCompletionSupport": true,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "deepseek-ai/DeepSeek-Coder-V2-Instruct",
        "deprecatedTime": {},
        "filling": "{\"属地\":\"北京市\",\"模型名称\":\"求索对话DeepSeek Chat\",\"备案单位\":\"北京深度求索人工智能基础技术研究有限公司\",\"备案号\":\"Beijing-DeepseekChat-202404280016\",\"备案时间\":\"2024/5/13\"}"
    },
    {
        "modelId": "17885302546",
        "modelName": "Pro/internlm/internlm2_5-7b-chat",
        "mf": "internlm",
        "desc": "InternLM2.5-7B-Chat 是一个开源的对话模型，基于 InternLM2 架构开发。该 7B 参数规模的模型专注于对话生成任务，支持中英双语交互。模型采用了最新的训练技术，旨在提供流畅、智能的对话体验。InternLM2.5-7B-Chat 适用于各种对话应用场景，包括但不限于智能客服、个人助手等领域",
        "tags": [
            "7B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/internlm.svg",
        "publishTime": {
            "seconds": 1719417600
        },
        "size": 7,
        "contextLen": 32768,
        "price": "0.35",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": false,
        "type": "text",
        "subType": "chat",
        "DisplayName": "InternLM2.5-7B-Chat(Pro)",
        "sort": 2,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 120000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 160000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 320000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 5000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "disable",
        "jsonModeSupport": true,
        "functionCallSupport": true,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "internlm/internlm2_5-7b-chat",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"上海市\", \"模型名称\": \"书生·浦语\", \"备案单位\": \"上海人工智能创新中心（上海人工智能实验室）\", \"备案号\": \"Shanghai-ShuShengPuYu-20230821\", \"备案时间\": \"2023/8/31\"}"
    },
    {
        "modelId": "17885302542",
        "modelName": "Pro/THUDM/chatglm3-6b",
        "mf": "THUDM",
        "desc": "ChatGLM3-6B 是 ChatGLM 系列的开源模型，由智谱 AI 开发。该模型保留了前代模型的优秀特性，如对话流畅和部署门槛低，同时引入了新的特性。它采用了更多样的训练数据、更充分的训练步数和更合理的训练策略，在 10B 以下的预训练模型中表现出色。ChatGLM3-6B 支持多轮对话、工具调用、代码执行和 Agent 任务等复杂场景。除对话模型外，还开源了基础模型 ChatGLM-6B-Base 和长文本对话模型 ChatGLM3-6B-32K。该模型对学术研究完全开放，在登记后也允许免费商业使用",
        "tags": [
            "6B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Zhipu.svg",
        "publishTime": {
            "seconds": 1698163200
        },
        "size": 6,
        "contextLen": 32768,
        "price": "0.35",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": false,
        "type": "text",
        "subType": "chat",
        "DisplayName": "chatglm3-6b (Pro)",
        "sort": 2,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 120000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 160000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 320000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 5000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "disable",
        "jsonModeSupport": true,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "THUDM/chatglm3-6b",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"北京市\", \"模型名称\": \"智谱清言（ChatGLM）\", \"备案单位\": \"北京智谱华章科技有限公司\", \"备案号\": \"Beijing-ChatGLM-20230821\", \"备案时间\": \"2023/8/31\"}"
    },
    {
        "modelId": "17885302623",
        "modelName": "Qwen/Qwen2.5-Coder-32B-Instruct",
        "mf": "Qwen2.5",
        "desc": "Qwen2.5-Coder-32B-Instruct 是基于 Qwen2.5 开发的代码特定大语言模型。该模型通过 5.5 万亿 tokens 的训练，在代码生成、代码推理和代码修复方面都取得了显著提升。它是当前最先进的开源代码语言模型，编码能力可与 GPT-4 相媲美。模型不仅增强了编码能力，还保持了在数学和通用能力方面的优势，并支持长文本处理",
        "tags": [
            "Coder",
            "32B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Tongyi.svg",
        "publishTime": {
            "seconds": 1731254400
        },
        "size": 32,
        "contextLen": 32768,
        "price": "1.26",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "Qwen2.5-Coder-32B-Instruct",
        "sort": 3200,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 40000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 60000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 160000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 2000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": true,
        "functionCallSupport": false,
        "fimCompletionSupport": true,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "Qwen/Qwen2.5-Coder-32B-Instruct",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"浙江省\", \"模型名称\": \"通义千问大模型\", \"备案单位\": \"阿里巴巴达摩院（杭州）科技有限公司\", \"备案号\": \"ZheJiang-TongYiQianWen-20230901\", \"备案时间\": \"2023/9/12\"}"
    },
    {
        "modelId": "17885302584",
        "modelName": "TeleAI/TeleChat2",
        "mf": "DianXin",
        "desc": "TeleChat2大模型是由中国电信从0到1自主研发的生成式语义大模型，支持百科问答、代码生成、长文生成等功能，为用户提供对话咨询服务，能够与用户进行对话互动，回答问题，协助创作，高效便捷地帮助用户获取信息、知识和灵感。模型在幻觉问题、长文生成、逻辑理解等方面均有较出色表现。",
        "tags": [
            "8K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Property%201%3D5G.svg",
        "publishTime": {
            "seconds": 1727280000
        },
        "size": 110,
        "contextLen": 8192,
        "price": "1.33",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "TeleChat2",
        "sort": 2199,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 20000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 30000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 40000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 250000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": false,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": false,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "TeleAI/TeleChat2",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"北京市\", \"模型名称\": \"星辰语义大模型\", \"备案单位\": \"中电信人工智能科技（北京）有限公司\", \"备案号\": \"Beijing-XingChen-202404280001\", \"备案时间\": \"2024/5/13\"}"
    },
    {
        "modelId": "17885302610",
        "modelName": "Pro/Qwen/Qwen2.5-Coder-7B-Instruct",
        "mf": "Qwen2.5",
        "desc": "Qwen2.5-Coder-7B-Instruct 是阿里云发布的代码特定大语言模型系列的最新版本。该模型在 Qwen2.5 的基础上，通过 5.5 万亿个 tokens 的训练，显著提升了代码生成、推理和修复能力。它不仅增强了编码能力，还保持了数学和通用能力的优势。模型为代码智能体等实际应用提供了更全面的基础",
        "tags": [
            "Coder",
            "7B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Tongyi.svg",
        "publishTime": {
            "seconds": 1726761600
        },
        "size": 7,
        "contextLen": 32768,
        "price": "0.35",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "Qwen2.5-Coder-7B-Instruct (Pro)",
        "sort": 230,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 120000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 160000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 320000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 5000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": true,
        "functionCallSupport": false,
        "fimCompletionSupport": true,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "Qwen/Qwen2.5-Coder-7B-Instruct",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"浙江省\", \"模型名称\": \"通义千问大模型\", \"备案单位\": \"阿里巴巴达摩院（杭州）科技有限公司\", \"备案号\": \"ZheJiang-TongYiQianWen-20230901\", \"备案时间\": \"2023/9/12\"}"
    },
    {
        "modelId": "17885302586",
        "modelName": "Pro/Qwen/Qwen2.5-7B-Instruct",
        "mf": "Qwen2.5",
        "desc": "Qwen2.5-7B-Instruct 是阿里云发布的最新大语言模型系列之一。该 7B 模型在编码和数学等领域具有显著改进的能力。该模型还提供了多语言支持，覆盖超过 29 种语言，包括中文、英文等。模型在指令跟随、理解结构化数据以及生成结构化输出（尤其是 JSON）方面都有显著提升",
        "tags": [
            "7B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Tongyi.svg",
        "publishTime": {
            "seconds": 1726588800
        },
        "size": 7,
        "contextLen": 32768,
        "price": "0.35",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "Qwen2.5-7B-Instruct (Pro)",
        "sort": 207,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 120000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 160000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 320000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 5000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": true,
        "functionCallSupport": true,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": true,
        "targetModelName": "Qwen/Qwen2.5-7B-Instruct",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"浙江省\", \"模型名称\": \"通义千问大模型\", \"备案单位\": \"阿里巴巴达摩院（杭州）科技有限公司\", \"备案号\": \"ZheJiang-TongYiQianWen-20230901\", \"备案时间\": \"2023/9/12\"}"
    },
    {
        "modelId": "17885302578",
        "modelName": "Qwen/Qwen2.5-Coder-7B-Instruct",
        "mf": "Qwen2.5",
        "desc": "Qwen2.5-Coder-7B-Instruct 是阿里云发布的代码特定大语言模型系列的最新版本。该模型在 Qwen2.5 的基础上，通过 5.5 万亿个 tokens 的训练，显著提升了代码生成、推理和修复能力。它不仅增强了编码能力，还保持了数学和通用能力的优势。模型为代码智能体等实际应用提供了更全面的基础",
        "tags": [
            "Coder",
            "7B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Tongyi.svg",
        "publishTime": {
            "seconds": 1726761600
        },
        "size": 7,
        "contextLen": 32768,
        "price": "0",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "Qwen2.5-Coder-7B-Instruct (Free)",
        "sort": 2550,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": true,
        "functionCallSupport": false,
        "fimCompletionSupport": true,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "Qwen/Qwen2.5-Coder-7B-Instruct",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"浙江省\", \"模型名称\": \"通义千问大模型\", \"备案单位\": \"阿里巴巴达摩院（杭州）科技有限公司\", \"备案号\": \"ZheJiang-TongYiQianWen-20230901\", \"备案时间\": \"2023/9/12\"}"
    },
    {
        "modelId": "17885302573",
        "modelName": "Qwen/Qwen2.5-7B-Instruct",
        "mf": "Qwen2.5",
        "desc": "Qwen2.5-7B-Instruct 是阿里云发布的最新大语言模型系列之一。该 7B 模型在编码和数学等领域具有显著改进的能力。该模型还提供了多语言支持，覆盖超过 29 种语言，包括中文、英文等。模型在指令跟随、理解结构化数据以及生成结构化输出（尤其是 JSON）方面都有显著提升",
        "tags": [
            "Free",
            "7B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Tongyi.svg",
        "publishTime": {
            "seconds": 1726588800
        },
        "size": 7,
        "contextLen": 32768,
        "price": "0",
        "currency": "¥",
        "operationLabel": [
            "HOT"
        ],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "Qwen2.5-7B-Instruct (Free)",
        "sort": 2570,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": true,
        "functionCallSupport": true,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": true,
        "targetModelName": "Qwen/Qwen2.5-7B-Instruct",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"浙江省\", \"模型名称\": \"通义千问大模型\", \"备案单位\": \"阿里巴巴达摩院（杭州）科技有限公司\", \"备案号\": \"ZheJiang-TongYiQianWen-20230901\", \"备案时间\": \"2023/9/12\"}"
    },
    {
        "modelId": "17885302572",
        "modelName": "Qwen/Qwen2.5-72B-Instruct",
        "mf": "Qwen2.5",
        "desc": "Qwen2.5-72B-Instruct 是阿里云发布的最新大语言模型系列之一。该 72B 模型在编码和数学等领域具有显著改进的能力。该模型还提供了多语言支持，覆盖超过 29 种语言，包括中文、英文等。模型在指令跟随、理解结构化数据以及生成结构化输出（尤其是 JSON）方面都有显著提升",
        "tags": [
            "72B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Tongyi.svg",
        "publishTime": {
            "seconds": 1726588800
        },
        "size": 72,
        "contextLen": 32768,
        "price": "4.13",
        "currency": "¥",
        "operationLabel": [
            "HOT"
        ],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "Qwen2.5-72B-Instruct",
        "sort": 2600,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 20000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 30000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 40000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 250000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": true,
        "functionCallSupport": true,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": true,
        "targetModelName": "Qwen/Qwen2.5-72B-Instruct",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"浙江省\", \"模型名称\": \"通义千问大模型\", \"备案单位\": \"阿里巴巴达摩院（杭州）科技有限公司\", \"备案号\": \"ZheJiang-TongYiQianWen-20230901\", \"备案时间\": \"2023/9/12\"}"
    },
    {
        "modelId": "17885302596",
        "modelName": "Pro/Qwen/Qwen2-VL-7B-Instruct",
        "mf": "Qwen2",
        "desc": "Qwen2-VL-7B-Instruct 是 Qwen-VL 模型的最新迭代版本，在视觉理解基准测试中达到了最先进的性能，包括 MathVista、DocVQA、RealWorldQA 和 MTVQA 等。Qwen2-VL 能够用于高质量的基于视频的问答、对话和内容创作，还具备复杂推理和决策能力，可以与移动设备、机器人等集成，基于视觉环境和文本指令进行自动操作。除了英语和中文，Qwen2-VL 现在还支持理解图像中不同语言的文本，包括大多数欧洲语言、日语、韩语、阿拉伯语和越南语等",
        "tags": [
            "7B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Tongyi.svg",
        "publishTime": {
            "seconds": 1724860800
        },
        "size": 7,
        "contextLen": 32768,
        "price": "0.35",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "Qwen2-VL-7B-Instruct (Pro)",
        "sort": 220,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 120000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 160000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 320000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 5000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": false,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": true,
        "vlm": true,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "Qwen/Qwen2-VL-7B-Instruct",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"浙江省\", \"模型名称\": \"通义千问大模型\", \"备案单位\": \"阿里巴巴达摩院（杭州）科技有限公司\", \"备案号\": \"ZheJiang-TongYiQianWen-20230901\", \"备案时间\": \"2023/9/12\"}"
    },
    {
        "modelId": "17885302535",
        "modelName": "internlm/internlm2_5-7b-chat",
        "mf": "internlm",
        "desc": "InternLM2.5-7B-Chat 是一个开源的对话模型，基于 InternLM2 架构开发。该 7B 参数规模的模型专注于对话生成任务，支持中英双语交互。模型采用了最新的训练技术，旨在提供流畅、智能的对话体验。InternLM2.5-7B-Chat 适用于各种对话应用场景，包括但不限于智能客服、个人助手等领域",
        "tags": [
            "Free",
            "7B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/internlm.svg",
        "publishTime": {
            "seconds": 1719417600
        },
        "size": 7,
        "contextLen": 32768,
        "price": "0",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "InternLM2.5-7B-Chat (Free)",
        "sort": 2170,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": true,
        "functionCallSupport": true,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "internlm/internlm2_5-7b-chat",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"上海市\", \"模型名称\": \"书生·浦语\", \"备案单位\": \"上海人工智能创新中心（上海人工智能实验室）\", \"备案号\": \"Shanghai-ShuShengPuYu-20230821\", \"备案时间\": \"2023/8/31\"}"
    },
    {
        "modelId": "17885302561",
        "modelName": "internlm/internlm2_5-20b-chat",
        "mf": "internlm",
        "desc": "InternLM2.5-20B-Chat 是一个开源的大规模对话模型，基于 InternLM2 架构开发。该模型拥有 200 亿参数，在数学推理方面表现出色，超越了同量级的 Llama3 和 Gemma2-27B 模型。InternLM2.5-20B-Chat 在工具调用能力方面有显著提升，支持从上百个网页收集信息进行分析推理，并具备更强的指令理解、工具选择和结果反思能力。它适用于构建复杂智能体，可进行多轮工具调用以完成复杂任务",
        "tags": [
            "20B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/internlm.svg",
        "publishTime": {
            "seconds": 1722268800
        },
        "size": 20,
        "contextLen": 32768,
        "price": "1",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "InternLM2.5-20B-Chat",
        "sort": 2180,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 40000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 60000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 160000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 2000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": true,
        "functionCallSupport": true,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "internlm/internlm2_5-20b-chat",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"上海市\", \"模型名称\": \"书生·浦语\", \"备案单位\": \"上海人工智能创新中心（上海人工智能实验室）\", \"备案号\": \"Shanghai-ShuShengPuYu-20230821\", \"备案时间\": \"2023/8/31\"}"
    },
    {
        "modelId": "17885302590",
        "modelName": "Qwen/Qwen2-VL-72B-Instruct",
        "mf": "Qwen2",
        "desc": "Qwen2-VL 是 Qwen-VL 模型的最新迭代版本，在视觉理解基准测试中达到了最先进的性能，包括 MathVista、DocVQA、RealWorldQA 和 MTVQA 等。Qwen2-VL 能够理解超过 20 分钟的视频，用于高质量的基于视频的问答、对话和内容创作。它还具备复杂推理和决策能力，可以与移动设备、机器人等集成，基于视觉环境和文本指令进行自动操作。除了英语和中文，Qwen2-VL 现在还支持理解图像中不同语言的文本，包括大多数欧洲语言、日语、韩语、阿拉伯语和越南语等",
        "tags": [
            "72B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Tongyi.svg",
        "publishTime": {
            "seconds": 1726502400
        },
        "size": 72,
        "contextLen": 32768,
        "price": "4.13",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "Qwen2-VL-72B-Instruct",
        "sort": 2790,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 20000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 30000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 40000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 250000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": false,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": true,
        "vlm": true,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "Qwen/Qwen2-VL-72B-Instruct",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"浙江省\", \"模型名称\": \"通义千问大模型\", \"备案单位\": \"阿里巴巴达摩院（杭州）科技有限公司\", \"备案号\": \"ZheJiang-TongYiQianWen-20230901\", \"备案时间\": \"2023/9/12\"}"
    },
    {
        "modelId": "17885302592",
        "modelName": "Pro/BAAI/bge-m3",
        "mf": "BAAI",
        "desc": "BGE-M3 是一个多功能、多语言、多粒度的文本嵌入模型。它支持三种常见的检索功能：密集检索、多向量检索和稀疏检索。该模型可以处理超过100种语言，并且能够处理从短句到长达8192个词元的长文档等不同粒度的输入。BGE-M3在多语言和跨语言检索任务中表现出色，在 MIRACL 和 MKQA 等基准测试中取得了领先结果。它还具有处理长文档检索的能力，在 MLDR 和 NarritiveQA 等数据集上展现了优秀性能",
        "tags": [
            "多语言",
            "1024 维",
            "8K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/BAAI.svg",
        "publishTime": {
            "seconds": 1706371200
        },
        "size": 0,
        "contextLen": 8192,
        "price": "0.07",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": false,
        "type": "text",
        "subType": "embedding",
        "DisplayName": "bge-m3 (Pro)",
        "sort": 208,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 2000,
                "TPM": 1000000,
                "IPM": 1,
                "IPD": 1440
            },
            {
                "Level": 1,
                "RPM": 3000,
                "TPM": 1000000,
                "IPM": 1,
                "IPD": 1440
            },
            {
                "Level": 2,
                "RPM": 5000,
                "TPM": 1000000,
                "IPM": 1,
                "IPD": 1440
            },
            {
                "Level": 3,
                "RPM": 5000,
                "TPM": 5000000,
                "IPM": 1,
                "IPD": 1440
            },
            {
                "Level": 4,
                "RPM": 10000,
                "TPM": 5000000,
                "IPM": 1,
                "IPD": 1440
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 10000000,
                "IPM": 1,
                "IPD": 1440
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": false,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": false,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "BAAI/bge-m3",
        "deprecatedTime": {},
        "filling": "{\"说明\": \"非生成式人工智能服务\"}"
    },
    {
        "modelId": "17885302562",
        "modelName": "Qwen/Qwen2-Math-72B-Instruct",
        "mf": "Qwen2",
        "desc": "Qwen2-Math-72B-Instruct 是 Qwen2 数学系列中的指令微调大语言模型，参数规模为 72B。该模型专门针对数学和算术问题解决能力进行了优化，在数学推理方面表现出色，超越了开源模型甚至一些闭源模型（如 GPT4）的数学能力。它基于 Qwen2 系列构建，旨在解决需要复杂、多步逻辑推理的高级数学问题。该模型目前主要支持英语，双语（英语和中文）版本将很快发布",
        "tags": [
            "Math",
            "72B",
            "4K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Tongyi.svg",
        "publishTime": {
            "seconds": 1723132800
        },
        "size": 72,
        "contextLen": 4096,
        "price": "4.13",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": false,
        "type": "text",
        "subType": "chat",
        "DisplayName": "【Deprecated】Qwen2-Math-72B-Instruct",
        "sort": 2,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 20000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 30000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 40000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 250000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "disable",
        "jsonModeSupport": true,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": false,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "Qwen/Qwen2-Math-72B-Instruct",
        "deprecatedTime": {},
        "filling": "{\"属地\":\"浙江省\",\"模型名称\":\"通义千问大模型\",\"备案单位\":\"阿里巴巴达摩院（杭州）科技有限公司\",\"备案号\":\"ZheJiang-TongYiQianWen-20230901\",\"备案时间\":\"2023/9/12\"}"
    },
    {
        "modelId": "17885302541",
        "modelName": "Pro/THUDM/glm-4-9b-chat",
        "mf": "THUDM",
        "desc": "GLM-4-9B-Chat 是智谱 AI 推出的 GLM-4 系列预训练模型中的开源版本。该模型在语义、数学、推理、代码和知识等多个方面表现出色。除了支持多轮对话外，GLM-4-9B-Chat 还具备网页浏览、代码执行、自定义工具调用（Function Call）和长文本推理等高级功能。模型支持 26 种语言，包括中文、英文、日语、韩语和德语等。在多项基准测试中，GLM-4-9B-Chat 展现了优秀的性能，如 AlignBench-v2、MT-Bench、MMLU 和 C-Eval 等。该模型支持最大 128K 的上下文长度，适用于学术研究和商业应用",
        "tags": [
            "9B",
            "128K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Zhipu.svg",
        "publishTime": {
            "seconds": 1717430400
        },
        "size": 9,
        "contextLen": 131072,
        "price": "0.6",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "glm-4-9b-chat (Pro)",
        "sort": 114,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 120000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 160000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 320000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 5000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": true,
        "functionCallSupport": true,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "THUDM/glm-4-9b-chat",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"北京市\", \"模型名称\": \"智谱清言（ChatGLM）\", \"备案单位\": \"北京智谱华章科技有限公司\", \"备案号\": \"Beijing-ChatGLM-20230821\", \"备案时间\": \"2023/8/31\"}"
    },
    {
        "modelId": "17885302522",
        "modelName": "THUDM/glm-4-9b-chat",
        "mf": "THUDM",
        "desc": "GLM-4-9B-Chat 是智谱 AI 推出的 GLM-4 系列预训练模型中的开源版本。该模型在语义、数学、推理、代码和知识等多个方面表现出色。除了支持多轮对话外，GLM-4-9B-Chat 还具备网页浏览、代码执行、自定义工具调用（Function Call）和长文本推理等高级功能。模型支持 26 种语言，包括中文、英文、日语、韩语和德语等。在多项基准测试中，GLM-4-9B-Chat 展现了优秀的性能，如 AlignBench-v2、MT-Bench、MMLU 和 C-Eval 等。该模型支持最大 128K 的上下文长度，适用于学术研究和商业应用",
        "tags": [
            "Free",
            "9B",
            "128K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Zhipu.svg",
        "publishTime": {
            "seconds": 1717430400
        },
        "size": 9,
        "contextLen": 131072,
        "price": "0",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "glm-4-9b-chat (Free)",
        "sort": 1914,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": true,
        "functionCallSupport": true,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "THUDM/glm-4-9b-chat",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"北京市\", \"模型名称\": \"智谱清言（ChatGLM）\", \"备案单位\": \"北京智谱华章科技有限公司\", \"备案号\": \"Beijing-ChatGLM-20230821\", \"备案时间\": \"2023/8/31\"}"
    },
    {
        "modelId": "17885302538",
        "modelName": "Pro/Qwen/Qwen2-7B-Instruct",
        "mf": "Qwen2",
        "desc": "Qwen2-7B-Instruct 是 Qwen2 系列中的指令微调大语言模型，参数规模为 7B。该模型基于 Transformer 架构，采用了 SwiGLU 激活函数、注意力 QKV 偏置和组查询注意力等技术。它能够处理大规模输入。该模型在语言理解、生成、多语言能力、编码、数学和推理等多个基准测试中表现出色，超越了大多数开源模型，并在某些任务上展现出与专有模型相当的竞争力。Qwen2-7B-Instruct 在多项评测中均优于 Qwen1.5-7B-Chat，显示出显著的性能提升",
        "tags": [
            "7B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Tongyi.svg",
        "publishTime": {
            "seconds": 1717430400
        },
        "size": 7,
        "contextLen": 32768,
        "price": "0.35",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "Qwen2-7B-Instruct (Pro)",
        "sort": 120,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 120000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 160000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 320000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 5000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": true,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "Qwen/Qwen2-7B-Instruct",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"浙江省\", \"模型名称\": \"通义千问大模型\", \"备案单位\": \"阿里巴巴达摩院（杭州）科技有限公司\", \"备案号\": \"ZheJiang-TongYiQianWen-20230901\", \"备案时间\": \"2023/9/12\"}"
    },
    {
        "modelId": "17885302539",
        "modelName": "Pro/Qwen/Qwen2-1.5B-Instruct",
        "mf": "Qwen2",
        "desc": "Qwen2-1.5B-Instruct 是 Qwen2 系列中的指令微调大语言模型，参数规模为 1.5B。该模型基于 Transformer 架构，采用了 SwiGLU 激活函数、注意力 QKV 偏置和组查询注意力等技术。它在语言理解、生成、多语言能力、编码、数学和推理等多个基准测试中表现出色，超越了大多数开源模型。与 Qwen1.5-1.8B-Chat 相比，Qwen2-1.5B-Instruct 在 MMLU、HumanEval、GSM8K、C-Eval 和 IFEval 等测试中均显示出显著的性能提升，尽管参数量略少",
        "tags": [
            "1.5B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Tongyi.svg",
        "publishTime": {
            "seconds": 1717344000
        },
        "size": 2,
        "contextLen": 32768,
        "price": "0.14",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "Qwen2-1.5B-Instruct (Pro)",
        "sort": 119,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 80000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1200,
                "TPM": 120000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 160000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 4000,
                "TPM": 320000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 8000,
                "TPM": 1000000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 10000,
                "TPM": 5000000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": true,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "Qwen/Qwen2-1.5B-Instruct",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"浙江省\", \"模型名称\": \"通义千问大模型\", \"备案单位\": \"阿里巴巴达摩院（杭州）科技有限公司\", \"备案号\": \"ZheJiang-TongYiQianWen-20230901\", \"备案时间\": \"2023/9/12\"}"
    },
    {
        "modelId": "17885302529",
        "modelName": "Qwen/Qwen2-1.5B-Instruct",
        "mf": "Qwen2",
        "desc": "Qwen2-1.5B-Instruct 是 Qwen2 系列中的指令微调大语言模型，参数规模为 1.5B。该模型基于 Transformer 架构，采用了 SwiGLU 激活函数、注意力 QKV 偏置和组查询注意力等技术。它在语言理解、生成、多语言能力、编码、数学和推理等多个基准测试中表现出色，超越了大多数开源模型。与 Qwen1.5-1.8B-Chat 相比，Qwen2-1.5B-Instruct 在 MMLU、HumanEval、GSM8K、C-Eval 和 IFEval 等测试中均显示出显著的性能提升，尽管参数量略少",
        "tags": [
            "Free",
            "1.5B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Tongyi.svg",
        "publishTime": {
            "seconds": 1717344000
        },
        "size": 2,
        "contextLen": 32768,
        "price": "0",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "Qwen2-1.5B-Instruct (Free)",
        "sort": 1919,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": true,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "Qwen/Qwen2-1.5B-Instruct",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"浙江省\", \"模型名称\": \"通义千问大模型\", \"备案单位\": \"阿里巴巴达摩院（杭州）科技有限公司\", \"备案号\": \"ZheJiang-TongYiQianWen-20230901\", \"备案时间\": \"2023/9/12\"}"
    },
    {
        "modelId": "17885302524",
        "modelName": "Qwen/Qwen2-7B-Instruct",
        "mf": "Qwen2",
        "desc": "Qwen2-7B-Instruct 是 Qwen2 系列中的指令微调大语言模型，参数规模为 7B。该模型基于 Transformer 架构，采用了 SwiGLU 激活函数、注意力 QKV 偏置和组查询注意力等技术。它能够处理大规模输入。该模型在语言理解、生成、多语言能力、编码、数学和推理等多个基准测试中表现出色，超越了大多数开源模型，并在某些任务上展现出与专有模型相当的竞争力。Qwen2-7B-Instruct 在多项评测中均优于 Qwen1.5-7B-Chat，显示出显著的性能提升",
        "tags": [
            "Free",
            "7B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Tongyi.svg",
        "publishTime": {
            "seconds": 1717430400
        },
        "size": 7,
        "contextLen": 32768,
        "price": "0",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "Qwen2-7B-Instruct (Free)",
        "sort": 1920,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": true,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "Qwen/Qwen2-7B-Instruct",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"浙江省\", \"模型名称\": \"通义千问大模型\", \"备案单位\": \"阿里巴巴达摩院（杭州）科技有限公司\", \"备案号\": \"ZheJiang-TongYiQianWen-20230901\", \"备案时间\": \"2023/9/12\"}"
    },
    {
        "modelId": "17885302519",
        "modelName": "THUDM/chatglm3-6b",
        "mf": "THUDM",
        "desc": "ChatGLM3-6B 是 ChatGLM 系列的开源模型，由智谱 AI 开发。该模型保留了前代模型的优秀特性，如对话流畅和部署门槛低，同时引入了新的特性。它采用了更多样的训练数据、更充分的训练步数和更合理的训练策略，在 10B 以下的预训练模型中表现出色。ChatGLM3-6B 支持多轮对话、工具调用、代码执行和 Agent 任务等复杂场景。除对话模型外，还开源了基础模型 ChatGLM-6B-Base 和长文本对话模型 ChatGLM3-6B-32K。该模型对学术研究完全开放，在登记后也允许免费商业使用",
        "tags": [
            "Free",
            "6B",
            "32K"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/Zhipu.svg",
        "publishTime": {
            "seconds": 1698163200
        },
        "size": 6,
        "contextLen": 32768,
        "price": "0",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": true,
        "type": "text",
        "subType": "chat",
        "DisplayName": "chatglm3-6b (Free)",
        "sort": 1913,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 1000,
                "TPM": 50000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": true,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": true,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "THUDM/chatglm3-6b",
        "deprecatedTime": {},
        "filling": "{\"属地\": \"北京市\", \"模型名称\": \"智谱清言（ChatGLM）\", \"备案单位\": \"北京智谱华章科技有限公司\", \"备案号\": \"Beijing-ChatGLM-20230821\", \"备案时间\": \"2023/8/31\"}"
    },
    {
        "modelId": "17885302537",
        "modelName": "BAAI/bge-large-zh-v1.5",
        "mf": "BAAI",
        "desc": "BAAI/bge-large-zh-v1.5 是一个大型中文文本嵌入模型，是 BGE (BAAI General Embedding) 系列的一部分。该模型在 C-MTEB 基准测试中表现出色，在 31 个数据集上的平均得分为 64.53，在检索、语义相似度、文本对分类等多个任务中都取得了优异成绩。它支持最大 512 个 token 的输入长度，适用于各种中文自然语言处理任务，如文本检索、语义相似度计算等",
        "tags": [
            "中文",
            "1024 维",
            "335M",
            "512"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/BAAI.svg",
        "publishTime": {
            "seconds": 1694448000
        },
        "size": 0,
        "contextLen": 512,
        "price": "0",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": false,
        "type": "text",
        "subType": "embedding",
        "DisplayName": "bge-large-zh-v1.5",
        "sort": 1801,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": false,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": false,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "BAAI/bge-large-zh-v1.5",
        "deprecatedTime": {},
        "filling": "{\"说明\": \"非生成式人工智能服务\"}"
    },
    {
        "modelId": "17885302536",
        "modelName": "BAAI/bge-large-en-v1.5",
        "mf": "BAAI",
        "desc": "BAAI/bge-large-en-v1.5 是一个大型英文文本嵌入模型，是 BGE (BAAI General Embedding) 系列的一部分。它在 MTEB 基准测试中取得了优异的表现，在 56 个数据集上的平均得分为 64.23，在检索、聚类、文本对分类等多个任务中表现出色。该模型支持最大 512 个 token 的输入长度，适用于各种自然语言处理任务，如文本检索、语义相似度计算等",
        "tags": [
            "英文",
            "1024 维",
            "335M",
            "512"
        ],
        "icon": "https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/Model_LOGO/BAAI.svg",
        "publishTime": {
            "seconds": 1694448000
        },
        "size": 0,
        "contextLen": 512,
        "price": "0",
        "currency": "¥",
        "operationLabel": [],
        "needAuth": false,
        "onlineDemo": false,
        "type": "text",
        "subType": "embedding",
        "DisplayName": "bge-large-en-v1.5",
        "sort": 1801,
        "sameSeries": [],
        "language": [],
        "modelInfo": [
            {
                "Level": 0,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 1,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 2,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 3,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 4,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            },
            {
                "Level": 5,
                "RPM": 2000,
                "TPM": 500000,
                "IPM": -1,
                "IPD": -1
            }
        ],
        "priceUnit": "/ M Tokens",
        "status": "normal",
        "jsonModeSupport": false,
        "functionCallSupport": false,
        "fimCompletionSupport": false,
        "chatPrefixCompletionSupport": false,
        "vlm": false,
        "onlyByChargeBalance": false,
        "supportFT": false,
        "targetModelName": "BAAI/bge-large-en-v1.5",
        "deprecatedTime": {},
        "filling": "{\"说明\": \"非生成式人工智能服务\"}"
    }
]